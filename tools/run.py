import threading
import time
from multiprocessing import Process, Queue

from hupu.community.base import get_article_list
from pools import spider
from tools.db import RedisClient


def run_get_article_list(q):
    # è™æ‰‘ç™»å½•åï¼Œäº§ç”Ÿçš„cookieï¼Œå¯¹äºè™æ‰‘è€Œè¨€uè¿™ä¸ªcookieæ˜¯å®é™…ä½¿ç”¨çš„key
    cookies = {
        "u": "28543539|6JCn55Gf5pyX|9b49|26d02c47a8424dd9fb9d72416a700969|a8424dd9fb9d7241|aHVwdV8wZWVlNTQ1YjA0NGY2OTlm"
    }
    for idx in range(2000, 0, -1):
        articles = get_article_list("vote", idx, cookies)
        print(f"{threading.get_ident()} - ç¬¬{idx}é¡µï¼Œå…±æŠ“å–{len(articles)}æ¡...")
        for row in articles:
            # çˆ¬å–æ–‡ç« ä¿¡æ¯
            article_data = {
                "type": "article",
                "data": {
                    "article_id": row['article_id']
                }
            }
            q.put(article_data)
            # çˆ¬å–æ–‡ç« çš„è¯„è®ºä¿¡æ¯
            for i in range(1, row['comment_pages'] + 1):
                comment_data = {
                    "type": "comment",
                    "data": {
                        "article_id": row['article_id'],
                        "page": i
                    }
                }
                q.put(comment_data)
                print(f"{threading.get_ident()} - æ–‡ç« {row['article_id']}æŠ“å–{row['comment_pages'] + 1}é¡µè¯„è®º...")
            if q.qsize() > 1500:
                print(f"{threading.get_ident()}  - é˜Ÿåˆ—é•¿åº¦ä¸º{q.qsize()}ï¼Œ æš‚åœ10åˆ†é’Ÿ...")
                time.sleep(600)
    q.put(None)
    print("2000é¡µå·²ç»åŠ è½½å®Œæˆ")


def run():
    q = Queue(2000)

    p = Process(target=run_get_article_list, args=(q,), name="get_article_list")
    p.start()

    for i in range(0, 2):
        p = Process(target=spider, args=(q,), name=f"spider-{i}")
        p.start()


def test_get_article_list():
    for idx in range(11, 12):
        # è™æ‰‘ç™»å½•åï¼Œäº§ç”Ÿçš„cookieï¼Œå¯¹äºè™æ‰‘è€Œè¨€uè¿™ä¸ªcookieæ˜¯å®é™…ä½¿ç”¨çš„key
        cookies = {
            "u": "28543539|6JCn55Gf5pyX|9b49|26d02c47a8424dd9fb9d72416a700969|a8424dd9fb9d7241|aHVwdV8wZWVlNTQ1YjA0NGY2OTlm"
        }
        articles = get_article_list("vote", idx, cookies)
        print(f"spider {idx} articles {len(articles)}...")
        for row in articles:
            # çˆ¬å–æ–‡ç« ä¿¡æ¯
            for i in range(1, row['comment_pages'] + 1):
                print(f"spider articles {row['article_id']} page {i} comment ...")


def test_extract_tags():
    import jieba
    from jieba.analyse import textrank, extract_tags
    contents = [
        # "éœå¤«å¾·æ˜¯å­£åèµ›ç»™å­—æ¯å‡†å¤‡çš„ç¤¼ç‰©",
        # "æˆ‘æ˜¯æœå…°ç‰¹ï¼Œ10000%ç•™é›·éœ†ï¼Œæœ‰é’±åˆæœ‰åï¼Œè¦å•¥åƒåœ¾ä¸¤è¿FMVP",
        # "åŒç†ï¼Œå¬ä½ ä»¬è¯´è©¹å§†æ–¯æ²¡æŠ€æœ¯ä¸ä¼šç½šçƒï¼ŒåºŸé˜Ÿå‹ï¼ŒåºŸæ•™ç»ƒï¼Œé‚£ä¹ˆä»–åˆæ˜¯æ€ä¹ˆæ··åˆ°ç°åœ¨å‘¢ï¼Ÿ",
        # "ä¼™å¤«å…»ç”Ÿç¯®çƒäº†å—ï¼Ÿ",
        # "ç›®å‰æ€»ç¯®æ¿æ•°9176ï¼ŒåŠ©æ”»æ•°9059ï¼Œæœ¬èµ›å­£åœºå‡ç¯®æ¿8ä¸ªï¼Œåœºå‡åŠ©æ”»10.7æ¬¡",
        # "æ¬§æ–‡ï¼šéº»æºœçš„ï¼ï¼ï¼æ’é˜ŸğŸ¶ğŸ¶ é“æ­‰ï¼!!",
        # "è¿™å°±æ˜¯å† å†›åå« å‡¯é‡Œæ¬§æ–‡ï¼ï¼",
        """è™æ‰‘4æœˆ11æ—¥è®¯ è¿‘æ—¥ï¼Œå¼€æ‹“è€…çƒæ˜Ÿè¾¾ç±³å®‰-åˆ©æ‹‰å¾·åœ¨ç›´æ’­ä¸­åˆ†äº«äº†è‡ªå·±ä¹‹å‰æ‰“æ£’çƒçš„è¶£äº‹ã€‚åˆ©æ‹‰å¾·è¯´ï¼šæˆ‘è·Ÿå¤§å®¶ä¹Ÿè°ˆè¿‡åˆ°ä¸å°‘æ¬¡ï¼Œå…¶å®æˆ‘ä¹‹å‰æ˜¯æ‰“æ£’çƒçš„ã€‚æˆ‘çˆ¸çˆ¸æ˜¯æ£’çƒè¿åŠ¨å‘˜ï¼Œå› æ­¤ä»–ä¹Ÿå¸Œæœ›æˆ‘ä»¬èƒ½å¤Ÿæˆä¸ºæ£’çƒæ‰‹ï¼Œæˆ‘ä»å°å°±å¼€å§‹æ‰“æ£’çƒï¼Œæˆ‘å°æ—¶å€™æ£’çƒæ‰“å¾—ä¸é”™ã€‚ä½†æ˜¯ä½ çŸ¥é“ä¸ºä»€ä¹ˆåæ¥æˆ‘æ‰“ç¯®çƒäº†ï¼Ÿæˆ‘å¾ˆå–œæ¬¢æ£’çƒï¼Œå¯æ˜¯æˆ‘å°æ—¶å€™æœ‰ä¸€æ¬¡æ‰“æ£’çƒè”èµ›ï¼Œç»“æœè¢«æŠ•æ‰‹æ‰”å‡ºçš„çƒç»™ç ¸åˆ°äº†ï¼Œç›´æ¥ç»™æˆ‘ç ¸æ”¾å¼ƒäº†ï¼Œæˆ‘è®°å¾—å½“æ—¶æˆ‘å°±ç›´æ¥ä»çƒåœºä¸Šé¢èµ°ä¸‹å»äº†ï¼Œå“ˆå“ˆå“ˆ""",
    ]
    # for i in contents:
    #     # print(textrank(i, withWeight=True))
    #     print(extract_tags(i, withWeight=True))
    # print("-------------------")

    jieba.load_userdict("F:\\mengxiang\\hupu_spider\\etc\\shh_dict.txt")
    jieba.analyse.set_idf_path('F:\\mengxiang\\hupu_spider\\tools\\kw_dict_gt_500.idf.txt')
    for i in contents:
        print(extract_tags(i, withWeight=True))
    print("-------------------")
    #     print(" ".join(jieba.cut(i)))
    # jieba.load_userdict('etc/shh_dict.txt')
    # for i in contents:
    #     print(" ".join(jieba.cut(i)))
    # for i in contents:
    #     print(textrank(i, withWeight=True))
    #     print(extract_tags(i, withWeight=True))


if __name__ == "__main__":
    test_extract_tags()